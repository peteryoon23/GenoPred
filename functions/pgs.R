#!/usr/bin/Rscript

# Identify score files (name and method combinations)
list_score_files <- function(config, quiet = F){

  combos<-NULL

  # Read in gwas_list
  gwas_list <- read_param(config = config, param = 'gwas_list', quiet = quiet)

  if(!is.null(gwas_list)){
    # Identify PGS methods to be included
    pgs_methods_list <- read_param(config = config, param = 'pgs_methods', return_obj = F, quiet = quiet)

    # Remove methods that are applied to groups of gwas
    pgs_methods_list <- pgs_methods_list[!(pgs_methods_list %in% pgs_group_methods)]

    combos <- rbind(combos,
                    expand.grid(name = gwas_list$name[gwas_list$pop == 'EUR'], method = pgs_methods_list))

    # List PGS methods applied to non-EUR populations
    pgs_methods_noneur <- pgs_methods_noneur[pgs_methods_noneur %in% pgs_methods_list]

    combos <- rbind(combos,
                    expand.grid(name = gwas_list$name[gwas_list$pop != 'EUR'], method = pgs_methods_noneur))
  }

  # Read in score_list
  score_list <- read_param(config = config, param = 'score_list', quiet = quiet)

  outdir <- read_param(config = config, param = 'outdir', return_obj = F, quiet = quiet)

  if(!is.null(score_list)){
    combos <- rbind(combos,
                    data.frame(
                      name = score_list$name,
                      method = 'external'))
  }

  # Read in gwas_groups
  gwas_groups <- read_param(config = config, param = 'gwas_groups', quiet = quiet)

  # Methods implemented when GWAS groups contains only 2 GWAS
  if(!is.null(gwas_groups)){
    # Identify gwas_groups containing >2 GWAS
    gwas_groups_2 <- gwas_groups[sapply(gwas_groups$gwas, function(x) sum(strsplit(x, ",")[[1]] != "") == 2), ]
    
    # Identify PGS methods to be included
    pgs_methods_list <- read_param(config = config, param = 'pgs_methods', return_obj = F, quiet = quiet)

    # Retain methods that are applied to groups with only 2 gwas
    pgs_methods_list <- pgs_methods_list[(pgs_methods_list %in% c('prscsx', 'xwing'))]
    
    # Provide combos for methods applied to groups of gwas
    combos <- rbind(combos, expand.grid(name = gwas_groups_2$name, method = pgs_methods_list))
    
    # For TL-PRS, list combos for tlprs_methods
    tlprs_methods<-read_param(config = config, param = 'tlprs_methods', return_obj = F, quiet = quiet)
    if(length(tlprs_methods) > 1 || !is.na(tlprs_methods)){
      combos <- rbind(combos, expand.grid(name = gwas_groups_2$name, method = paste0('tlprs_', tlprs_methods)))
    }
    
    # For LEOPARD, list combos for leopard_methods
    leopard_methods<-read_param(config = config, param = 'leopard_methods', return_obj = F, quiet = quiet)
    if(length(leopard_methods) > 1 || !is.na(leopard_methods)){
      combos <- rbind(combos, expand.grid(name = gwas_groups_2$name, method = paste0(leopard_methods,'_multi')))
    }
  }

  # Methods implemented when GWAS groups contain >2 GWAS
  if(!is.null(gwas_groups)){
    # Identify gwas_groups with more than 2 gwas
    gwas_groups_more <- gwas_groups[sapply(gwas_groups$gwas, function(x) sum(strsplit(x, ",")[[1]] != "") > 2), ]
    
    # Identify PGS methods to be included
    pgs_methods_list <- read_param(config = config, param = 'pgs_methods', return_obj = F, quiet = quiet)
    
    # Retain methods that are applied to groups with only 2 gwas
    pgs_methods_list <- pgs_methods_list[(pgs_methods_list %in% c('prscsx'))]
    
    # Provide combos for methods applied to groups of gwas
    combos <- rbind(combos, expand.grid(name = gwas_groups_more$name, method = pgs_methods_list))
    
    # For LEOPARD, list combos for leopard_methods
    leopard_methods<-read_param(config = config, param = 'leopard_methods', return_obj = F)
    if(length(leopard_methods) > 1 || !is.na(leopard_methods)){
      combos <- rbind(combos, expand.grid(name = gwas_groups_more$name, method = paste0(leopard_methods,'_multi')))
    }
  }
  
  combos <- data.table(combos)
  combos <- combos[, lapply(.SD, as.character)]
  
  # Remove score files where .score.gz does not exist
  combos_keep <- NULL
  for(i in 1:nrow(combos)){
    score_i <- paste0(outdir, '/reference/pgs_score_files/', combos$method[i],'/', combos$name[i],'/ref-',combos$name[i], '.score.gz')
    if(file.exists(score_i)){
      combos_keep <- rbind(combos_keep, combos[i,])
    } else {
      if(quiet == F){
        cat0('No score file present for ', combos$method[i],' - ', combos$name[i],'. Check logs for reason.\n')
      }
    }
  }
  
  return(combos_keep)
}

# Flip effects in score file to match A1 reference
map_score<-function(ref, score){
  # Check if required columns exist
  required_cols <- c('SNP', 'A1', 'A2')
  if (!all(required_cols %in% names(ref)) | 
      !all(required_cols %in% names(score))) {
    stop('ref and score must contain SNP, A1, and A2 columns.')
  }
  
  # Valid alleles
  valid_alleles <- c('A', 'T', 'C', 'G')
  
  # Check for NA or invalid alleles in A1 and A2 columns
  for (col in c('A1', 'A2')) {
    if (any(is.na(ref[[col]])) | any(!ref[[col]] %in% valid_alleles)) {
      stop(paste('Invalid allele values detected in ref column:', col))
    }
    if (any(is.na(score[[col]])) | any(!score[[col]] %in% valid_alleles)) {
      stop(paste('Invalid allele values detected in score column:', col))
    }
  }
  
  # Check for NA values in SNP column
  if (any(is.na(ref$SNP)) | any(is.na(score$SNP))) {
    stop('NA values detected in SNP column of ref or score.')
  }
  
  ref <- ref[, c('SNP','A1','A2'), with = F]
  
  ref$IUPAC <- snp_iupac(ref$A1, ref$A2)
  score$IUPAC <- snp_iupac(score$A1, score$A2)
  
  tmp <- merge(ref, score, by = c('SNP','IUPAC'), all.x=T, sort = F)
  flip <- which(tmp$A1.x != tmp$A1.y)
  tmp <- as.matrix(tmp[, -1:-6, drop = FALSE])
  tmp[flip, ] <- -tmp[flip, ,drop=F]
  ref$IUPAC <-NULL
  new_score<-cbind(ref, tmp)
  new_score[is.na(new_score)]<-0

  if(nrow(new_score) != nrow(ref)){
    stop('Mapped score file has different number of rows to reference\n')
  }

  return(new_score)
}

# Calculate mean and sd of scores in file with plink .sscore format
score_mean_sd<-function(scores, keep=NULL){
    if(!is.null(keep)){
        scores<-scores[paste0(scores$FID, '_', scores$IID) %in% paste0(keep$FID, '_', keep$IID),]
    }

    scale<-data.table(  Param=names(scores)[-1:-2],
                        Mean=sapply(scores[,-1:-2, with=F], function(x) mean(x)),
                        SD=sapply(scores[,-1:-2, with=F], function(x) sd(x)))

    return(scale)
}

# Scale the target scores based on the reference mean and sd
score_scale<-function(score, ref_scale){
    for(i in ref_scale$Param){
        score[[i]]<-score[[i]]-ref_scale$Mean[ref_scale$Param == i]
        score[[i]]<-score[[i]]/ref_scale$SD[ref_scale$Param == i]
        score[[i]]<-round(score[[i]],3)
    }
    return(score)
}

# Read in SNP data from either plink1 binary, bgen or vcf
read_geno <- function(target, format) {
  if (!all(format %in% c('plink1', 'plink2', 'bgen', 'vcf'))) {
    stop('Specified format must be either plink1, plink2, bgen, or vcf.')
  }

  if (format == 'plink1') {
    target_snp <- fread(paste0(target, '.bim'))
    target_snp$V3 <- NULL
    names(target_snp) <- c('CHR', 'SNP', 'BP', 'A1', 'A2')
  }

  if (format == 'plink2') {
    target_snp <- fread(paste0(target, '.pvar'))
    names(target_snp)[1:5] <- c('CHR', 'BP', 'SNP', 'A1', 'A2')
  }

  if (format == 'bgen') {
    connection = dbConnect(RSQLite::SQLite(), paste0(target, '.bgen.bgi'))
    target_snp = dbGetQuery(connection, "SELECT * FROM Variant")
    target_snp <-
      target_snp[, c('chromosome', 'rsid', 'position', 'allele1', 'allele2')]
    names(target_snp) <- c('CHR', 'SNP', 'BP', 'A1', 'A2')
    dbDisconnect(connection)
    target_snp <- data.table(target_snp)
    target_snp$CHR <- as.numeric(gsub('chr', '', target_snp$CHR))
  }

  if (format == 'vcf') {
    target_snp <- fread(cmd = paste0("zcat ", target, ".vcf.gz | cut -f 1-5"), skip='#CHROM')
    names(target_snp) <- c('CHR', 'BP', 'SNP', 'A1', 'A2')
    target_snp$CHR <- as.numeric(gsub('chr', '', target_snp$CHR))
  }

  target_snp <- target_snp[, c('CHR', 'BP', 'SNP', 'A1', 'A2'), with = F]

  return(target_snp)
}

# Read in PLINK .bim file
read_bim<-function(dat, chr = 1:22){
  bim<-NULL
  for(i in chr){
    bim<-rbind(bim, fread(paste0(dat, i,'.bim')))
  }
  bim<-bim[,c('V1','V2','V4','V5','V6')]
  names(bim)<-c('CHR','SNP','BP','A1','A2')

  return(bim)
}

# Read in reference pop_data
read_pop_data <- function(x){
  pop_data<-fread(x)
  names(pop_data)<-gsub('\\#', '', names(pop_data))
  if (ncol(pop_data) == 2) {
    pop_data <- data.table(FID = pop_data$IID,
                           IID = pop_data$IID,
                           POP = pop_data$POP)
  } else {
    pop_data <- data.table(FID = pop_data$FID,
                           IID = pop_data$IID,
                           POP = pop_data$POP)
  }
  return(pop_data)
}

# Read in PLINK2 .pvar file
read_pvar<-function(dat, chr = 1:22){
  pvar<-NULL
  for(i in chr){
    pvar<-rbind(pvar, fread(paste0(dat, i,'.pvar')))
  }
  names(pvar)<-c('CHR','BP','SNP','A2','A1')

  return(pvar)
}

# Read in the .freq file for target population
read_frq<-function(freq_dir, population, chr){
  freq_data<-NULL
  for(i in chr){
    tmp<-fread(paste0(freq_dir,'/',population,'/ref.',population,'.chr',i,'.afreq'))
    tmp<-data.table(
      SNP = tmp$ID,
      A1 = tmp$ALT,
      A2 = tmp$REF,
      FREQ = tmp$ALT_FREQS
    )
    freq_data<-rbind(freq_data, tmp)
  }
  return(freq_data)
}

# Remove variants within genomic regions (REF: PMC2443852)
remove_regions<-function(dat, regions){
  exclude<-NULL
  for(i in 1:nrow(regions)){
    exclude<-c(exclude, dat$SNP[(   dat$CHR == regions$CHR[i]  &
                                    dat$BP >= regions$P0[i] &
                                    dat$BP <= regions$P1[i])])
  }

  return(dat[!(dat$SNP %in% exclude),])
}

# Read in GWAS summary statistics
read_sumstats<-function(sumstats, chr = 1:22, log_file = NULL, extract = NULL, req_cols = c('CHR','BP','A1','A2','BETA','SE','P','FREQ')){
  gwas<-fread(sumstats)

  log_add(log_file = log_file, message = paste0('sumstats contains ',nrow(gwas),' variants.'))

  # Retain requested chromosomes
  if(!all(1:22 %in% chr)){
    if(!('CHR' %in% names(gwas))){
      stop('Cannot filter sumstats by chromosome when CHR column is not present.')
    }
    gwas <- gwas[gwas$CHR %in% chr,]
    log_add(log_file = log_file, message = paste0(nrow(gwas), ' variants remain after selecting chromosomes.'))
  }

  # If FREQ is missing, use REF.FREQ
  if('FREQ' %in% req_cols){
    if(all(names(gwas) != 'FREQ')){
      gwas$FREQ <- gwas$REF.FREQ
      log_add(log_file = log_file, message = 'REF.FREQ being used as FREQ.')
    }
  }

  # Check for essential columns
  if(!(all(req_cols %in% names(gwas)))){
    stop(paste0('Required column/s missing in sumstats: ', paste(!(names(gwas) %in% req_cols), collapse = ', ')))
  }

  # Remove non-essential columns
  gwas <- gwas[, req_cols, with = F]

  # Remove rows with missing data
  gwas <- gwas[complete.cases(gwas),]

  log_add(log_file = log_file, message = paste0('sumstats contains ',nrow(gwas),' variants with complete data.'))

  if(!is.null(extract)){
    extract <- obj_or_file(extract, return_file = F)
    gwas<-gwas[(gwas$SNP %in% extract),]
    log_add(log_file = log_file, message = paste0('After applying the extract file, ',nrow(gwas),' variants remain.'))
  }

  return(gwas)
}

# Run LDSC
ldsc <- function(sumstats, ldsc, hm3_snplist, munge_sumstats, ld_scores, log_file = NULL){
  tmp_dir<-tempdir()

  # Munge the sumstats
  system(paste0(munge_sumstats, ' --sumstats ', sumstats,' --merge-alleles ', hm3_snplist, ' --out ', tmp_dir,'/munged'))

  # Define the file paths
  sumstats_path <- file.path(tmp_dir,'/munged.sumstats.gz')
  output_path <- file.path(tmp_dir, '/ldsc')

  # Run LDSC
  system(paste0(ldsc, ' --h2 ', sumstats_path, ' --ref-ld ', ld_scores, ' --w-ld ', ld_scores, ' --out ', output_path))

  # Read the log file
  ldsc_log <- readLines(paste0(tmp_dir, '/ldsc.log'))
  
  # Extract Total Observed hÂ² and SE
  h2_line <- grep("^Total Observed scale h2:", ldsc_log, value = TRUE)
  h2_parts <- regmatches(h2_line, regexec("h2:\\s+([0-9.]+)\\s+\\(([0-9.]+)\\)", h2_line))[[1]]
  h2 <- as.numeric(h2_parts[2])
  h2_se <- as.numeric(h2_parts[3])
  
  # Extract lambda GC
  lambda_line <- grep("^Lambda GC:", ldsc_log, value = TRUE)
  lambda_gc <- as.numeric(sub("Lambda GC:\\s+", "", lambda_line))
  
  # Extract intercept and its SE
  intercept_line <- grep("^Intercept:", ldsc_log, value = TRUE)
  intercept_parts <- regmatches(intercept_line, regexec("Intercept:\\s+([0-9.]+)\\s+\\(([0-9.]+)\\)", intercept_line))[[1]]
  intercept <- as.numeric(intercept_parts[2])
  intercept_se <- as.numeric(intercept_parts[3])
  
  # Return as a data frame
  results <- data.frame(
    h2 = h2,
    h2_se = h2_se,
    lambda_gc = lambda_gc,
    intercept = intercept,
    intercept_se = intercept_se
  )
  
  # Log the heritability estimate
  log_add(log_file = log_file, message = paste0('SNP-heritability estimate on the observed scale = ', results$h2, " (", results$h2_se, ")."))
  log_add(log_file = log_file, message = paste0('LDSC intercept = ', results$intercept, " (", results$intercept_se, ")."))
  log_add(log_file = log_file, message = paste0('Lambda GC = ', results$lambda_gc, "."))
  
  return(results)
}

# Match A1 and A2 match a reference
allele_match<-function(sumstats, ref_bim, chr = 1:22){
  sumstats_ref<-merge(sumstats, ref_bim[, c('SNP','A1','A2'), with=F], by = 'SNP')
  swap_index <- sumstats_ref$A1.x == sumstats_ref$A2.y & sumstats_ref$A2.x == sumstats_ref$A1.y
  sumstats_ref$BETA[swap_index] <- -sumstats_ref$BETA[swap_index]
  sumstats_ref$FREQ[swap_index] <- 1-sumstats_ref$FREQ[swap_index]

  names(sumstats_ref)[names(sumstats_ref) == 'A1.y'] <- 'A1'
  names(sumstats_ref)[names(sumstats_ref) == 'A2.y'] <- 'A2'
  sumstats_ref<-sumstats_ref[, names(sumstats), with = F]

  return(sumstats_ref)
}

# Run DBSLMM
dbslmm <- function(dbslmm, plink = plink, ld_blocks, chr = 1:22, bfile, sumstats, h2, h2f = 1, nsnp, nindiv, log_file = NULL, ncores=1){
  # Create temp directory
  tmp_dir<-tempdir()

  # Make sure there is permission to run dbslmm
  system(paste0('chmod 777 ', dbslmm))

  # Run dbslmm for each chromosome
  for(chr_i in chr){
    cmd <-paste0('Rscript ', dbslmm,'/DBSLMM.R --plink ', plink,' --type t --block ', ld_blocks,'/fourier_ls-chr', chr_i, '.bed --dbslmm ', dbslmm, '/dbslmm --model DBSLMM --h2 ', h2,' --h2f ', paste(h2f, collapse = ','),' --ref ', bfile, chr_i, ' --summary ', sumstats, chr_i, '.assoc.txt --n ', nindiv,' --nsnp ', nsnp, ' --outPath ', tmp_dir, '/ --thread ', ncores)
    exit_status <- system(cmd, intern = F)
  }

  # Read in DBSLMM output
  for(h2f_i in h2f){
    dbslmm_output_i <- list.files(path=tmp_dir, pattern=paste0('h2f', h2f_i, '.dbslmm.txt'))
    if(length(dbslmm_output_i) != 22){
      log_add(log_file = log_file, message = paste0('At least one chromosome did not complete with h2f of', h2f_i, '.'))
    }

    dbslmm_all_i<-NULL
    for(file_i in dbslmm_output_i){
      dbslmm_all_i<-rbind(dbslmm_all_i, fread(paste0(tmp_dir,'/',file_i)))
    }

    if(h2f_i == h2f[1]){
      dbslmm_all<-dbslmm_all_i[,c(1,2,4), with=T]
      names(dbslmm_all)<-c('SNP', 'A1', paste0('SCORE_DBSLMM_',h2f_i))
    } else {
      dbslmm_all_i<-dbslmm_all_i[,c(1,4), with=T]
      names(dbslmm_all_i)<-c('SNP', paste0('SCORE_DBSLMM_',h2f_i))
      dbslmm_all <- merge(dbslmm_all, dbslmm_all_i, by = 'SNP')
    }
  }

  # Insert A2 information
  bim<-read_bim(bfile, chr = chr)
  dbslmm_all<-merge(dbslmm_all, bim[, c('SNP','A1','A2'), with = F], by = c('SNP','A1'), all.x = T)
  if(any(is.na(dbslmm_all$A2))){
    stop('Insertion of A2 in dbslmm score file failed.')
  }

  dbslmm_all <- dbslmm_all[, c('SNP','A1','A2',names(dbslmm_all)[grepl('SCORE_DBSLMM', names(dbslmm_all))]), with=F]

  return(dbslmm_all)
}

# Calculate predictor correlations using LDAK
ldak_pred_cor<-function(bfile, ldak, n_cores, chr = 1:22, keep = NULL){
  tmp_dir<-tempdir()
  tmp_file<-tempfile()

  ldak_opt <- NULL
  if(!is.null(keep)){
    ldak_opt <- paste0(ldak_opt, '--keep ', keep, ' ')
  }
  if(length(chr) != 1){
    for(chr_i in chr){
      system(paste0(ldak, ' --calc-cors ', tmp_dir, '/cors_full', chr_i, ' --bfile ', bfile, ' ', ldak_opt,'--window-cm 3 --chr ', chr_i, ' --max-threads ', n_cores))
    }
    write.table(paste0(tmp_dir, '/cors_full', chr), paste0(tmp_dir, '/cors_list.txt'), col.names=F, row.names=F, quote=F)
    system(paste0(ldak,' --join-cors ', tmp_file, ' --corslist ', tmp_dir, '/cors_list.txt --max-threads ', n_cores))
  } else {
    system(paste0(ldak,' --calc-cors ', tmp_file, ' --bfile ', bfile, ' ', ldak_opt,'--window-cm 3 --chr ', chr, ' --max-threads ', n_cores))
  }

  return(tmp_file)
}

# Read in external score file
read_score <- function(score, chr = 1:22, log_file = NULL){
	# Read in score file
	score <- fread(score)

	log_add(log_file = log_file, message = paste0('Original score file contains ', nrow(score),' variants.'))

	# Remove variants with no effect
	score <- score[score$effect_weight != 0,]
	log_add(log_file = log_file, message = paste0('Score file contains ',nrow(score),' variants after removing variant with effect size of zero.'))
	n_snp_orig <- nrow(score)
	
	# Check whether harmonised columns present
	if(any(c("hm_rsID", "hm_chr", "hm_pos") %in% names(score))){
		log_add(log_file = log_file, message = 'PGSC harmonisation data present.')

    # If other_allele is not present in score file, try to use inferred other allele data in score file
    if(all(names(score) != 'other_allele')){
      # If hm_inferOtherAllele is all NA, see if we can get from variant_description column
      if(all(is.na(score$hm_inferOtherAllele))){
        if('variant_description' %in% names(score)){
          # Check whether variant_description contains chr:bp:a1:a2 information
          valid <- any(grepl("^([0-9XYMT]+):([0-9]+):([ACGT]):([ACGT])$", score$variant_description))
          if(valid){
            var_info<-data.table(do.call(rbind, strsplit(score$variant_description, ':')))
            names(var_info)<-c('CHR','BP','A1','A2')
            score$other_allele<-NA
            score$other_allele[score$effect_allele == var_info$A1] <- var_info$A2[score$effect_allele == var_info$A1]
            score$other_allele[score$effect_allele == var_info$A2] <- var_info$A1[score$effect_allele == var_info$A2]
            log_add(log_file = log_file, message = 'A2 information is missing so inferred from variant_description column.')
            score
          } else {
            log_add(log_file = log_file, message = 'A2 information is not present.')
            stop('A2 information is not present.')
          }
        }
      } else {
        score$other_allele <- score$hm_inferOtherAllele
        log_add(log_file = log_file, message = 'A2 is missing so using PGSC inferred A2.')
      }
    }
    
	  # Remove variants that were not mapped during harmonisation
	  score <- score[!is.na(score$hm_pos),]
	  log_add(log_file = log_file, message = paste0('Score file contains ',nrow(score),' variants after removing variants that were not mapped during PGSC harmonisation.'))
	  
	  # Remove variants that have no A2 information
	  score <- score[score$other_allele != '',]
	  log_add(log_file = log_file, message = paste0('Score file contains ',nrow(score),' variants missing inferred A2 information'))
	  
	  # Remove variants with multiple inferred A2 alleles
	  score <- score[!grepl('/', score$other_allele), ]
	  log_add(log_file = log_file, message = paste0('Score file contains ',nrow(score),' variants after removing multi-allelic variants with unspecified A2 allele.'))
	  
		# Relabel header
		score <- score[, names(score) %in% c('hm_rsID','hm_chr','hm_pos','effect_allele','other_allele','effect_weight'), with = F]
		names(score)[names(score) == 'hm_rsID'] <- 'SNP'
		names(score)[names(score) == 'hm_chr'] <- 'CHR'
		names(score)[names(score) == 'hm_pos'] <- 'BP'
		names(score)[names(score) == 'effect_allele'] <- 'A1'
		names(score)[names(score) == 'other_allele'] <- 'A2'

	} else {
		# Relabel header
		score <- score[, names(score) %in% c('rsID','chr_name','chr_position','effect_allele','other_allele','effect_weight'), with = F]
		names(score)[names(score) == 'rsID'] <- 'SNP'
		names(score)[names(score) == 'chr_name'] <- 'CHR'
		names(score)[names(score) == 'chr_position'] <- 'BP'
		names(score)[names(score) == 'effect_allele'] <- 'A1'
		names(score)[names(score) == 'other_allele'] <- 'A2'
	}

  if(!('SNP' %in% names(score)) & !(all(c('CHR','BP') %in% names(score)))){
    log_add(log_file = log_file, message = 'Either SNP, or CHR and BP data must be present in the score file')
    stop('Either SNP, or CHR and BP data must be present in the score file.')
  }

  if(!(all(c('A1','A2') %in% names(score)))){
    log_add(log_file = log_file, message = 'Both A1 and A2 data must be present in the score file')
    stop('Both A1 and A2 data must be present in the score file.')
  }

	if(any(names(score) == 'CHR')){
	  # Remove for 'chr' string in CHR column
	  score$CHR <- gsub('chr', '', score$CHR)
	  score <- score[score$CHR %in% chr,]
	  log_add(log_file = log_file, message = paste0('Score file contains ',nrow(score),' variants after removing those with chromosome not in: ', paste(chr, collapse = ', ')))
	}
	
	# Remove columns that are all na
	score <- score[, apply(score, 2, function(x) !all(is.na(x))), with = F]
	
	# Remove duplicate variants, retaining the row with larger effect
	score<-score[rev(order(abs(score$effect_weight))),]
	score$IUPAC <- snp_iupac(score$A1, score$A2)
	if(all(c('CHR','BP') %in% names(score))){
  	score<-score[!duplicated(paste0(score$CHR,':', score$BP,':', score$IUPAC)),]
  } else {
    score<-score[!duplicated(paste0(score$SNP,':', score$IUPAC)),]
  }
	score$IUPAC <- NULL
	
	log_add(log_file = log_file, message = paste0('Score file contains ',nrow(score),' variants after removing duplicates.'))
	
	return(list(n_snp_orig = n_snp_orig, score = score))
}


quickprs<-function(sumstats, quickprs_ldref, quickprs_multi_ldref = NULL, genomic_control, prs_model, n_cores = 1, ref_subset = NULL){
  tmp_dir<-tempfile()
  dir.create(tmp_dir)
  
  # Check if quickprs_multi_ldref and ref_subset are both NULL or both non-NULL
  if (xor(is.null(quickprs_multi_ldref), is.null(ref_subset))) {
    stop("Both 'quickprs_multi_ldref' and 'ref_subset' must either be NULL or non-NULL.")
  }
  
  ######
  # Estimate Per-Predictor Heritabilities
  ######

  # Calculate Per-Predictor Heritabilities.
  quickprs_ldref_files<-list.files(quickprs_ldref)

  tagging_file<-quickprs_ldref_files[grepl('quickprs.tagging',quickprs_ldref_files)]
  matrix_file<-quickprs_ldref_files[grepl('quickprs.matrix',quickprs_ldref_files)]

  if(opt$genomic_control == F){
    system(paste0(opt$ldak,' --sum-hers ', tmp_dir, '/bld.ldak --tagfile ', quickprs_ldref, '/', tagging_file, ' --summary ', sumstats, ' --matrix ', quickprs_ldref, '/', matrix_file, ' --max-threads ', n_cores, ' --check-sums NO'))
  } else{
    system(paste0(opt$ldak,' --sum-hers ', tmp_dir, '/bld.ldak --genomic-control YES --tagfile ', quickprs_ldref, '/', tagging_file, ' --summary ', sumstats, ' --matrix ', quickprs_ldref, '/', matrix_file, ' --max-threads ', n_cores, ' --check-sums NO'))
  }

  ldak_res_her<-fread(paste0(tmp_dir,'/bld.ldak.hers'))

  ######
  # Estimate effect sizes for training and full prediction models.
  ######

  if(!is.null(ref_subset)){
    quickprs_multi_ldref_files<-list.files(quickprs_multi_ldref)
    ref_dir <- quickprs_multi_ldref
    cor_file_prefix<-gsub('.cors.bin','',quickprs_multi_ldref_files[grepl(paste0('subset_', ref_subset, '.cors.bin'),quickprs_multi_ldref_files)])
  } else {
    cor_file_prefix<-gsub('.cors.bin','',quickprs_ldref_files[grepl('.cors.bin',quickprs_ldref_files) & !grepl('subset', quickprs_ldref_files)])
    ref_dir <- quickprs_ldref
  }

  system(paste0(opt$ldak,' --mega-prs ',tmp_dir,'/mega_full --model ', prs_model,' --cors ', ref_dir, '/', cor_file_prefix, ' --ind-hers ', tmp_dir, '/bld.ldak.ind.hers --summary ', sumstats, ' --high-LD ', quickprs_ldref, '/highld.snps --cv-proportion 0.1 --window-cm 1 --max-threads ', n_cores,' --extract ', sumstats))

  # Identify the best fitting model
  ldak_res_cors <- fread(paste0(tmp_dir, '/mega_full.cors'), nThread = n_cores)
  best_score <- ldak_res_cors[which.max(ldak_res_cors$Correlation),]

  ######
  # Format final score file
  ######

  # Read in the scores
  score <- fread(paste0(tmp_dir,'/mega_full.effects'), nThread = n_cores)
  score <- score[, c(1, 2, 3, 5), with = F]
  names(score) <- c('SNP','A1','A2','BETA')

  return(score)
}

# Derive trans-ancestry PGS models and estimate PGS residual scale
model_trans_pgs<-function(scores=NULL, pcs=NULL, output=NULL){
  if(any(is.null(c(scores, pcs, output)))){
    stop('Error: All parameters must be specified.')
  }
  
  if(is.character(pcs)){
    # Read in the reference PCs, extract PC columns, and update headers
    pcs_dat<-fread(pcs)
    names(pcs_dat)[1]<-'FID'
    pcs_dat<-pcs_dat[,grepl('FID|IID|^PC', names(pcs_dat)), with=F]
  } else {
    pcs_dat<-pcs
  }
  
  # Merge PGS and PCs
  scores_pcs<-merge(scores, pcs_dat, by=c('FID','IID'))
  
  # Calculate PGS residuals
  pcs_noid<-scores_pcs[,grepl('^PC', names(scores_pcs)), with=F]
  
  mod_list<-NULL
  scores_pcs_resid<-scores_pcs
  for(i in names(scores)[-1:-2]){
    mod_list[[i]]<-list()
    
    tmp<-data.table(y=scores_pcs[[i]], pcs_noid)
    
    # Model differences in mean
    pgs_pc_mean_mod<-lm(y ~ ., data=tmp)
    
    # Model differences in variance of residuals
    # Use gamma distribution to constrain predicted variance to be non-negative
    predicted_pgs <- predict(pgs_pc_mean_mod, newdata = tmp)
    residual_pgs <- tmp$y - predicted_pgs
    squared_residuals <- residual_pgs^2
    squared_residuals <- pmax(squared_residuals, 1e-4)

    pgs_pc_var_mod <- glm2(squared_residuals ~ ., data = tmp[, names(tmp) != 'y', with=F], family = Gamma(link = "log"))
    predicted_pgs_var <- exp(predict(pgs_pc_var_mod, newdata = tmp))
    
    scores_pcs_resid[[i]]<-residual_pgs/sqrt(predicted_pgs_var)
    
    mod_list[[i]]$mean_model <- compact_lm(pgs_pc_mean_mod)
    mod_list[[i]]$var_model <- compact_lm(pgs_pc_var_mod)
  }
  
  scores_pcs_resid<-scores_pcs_resid[,grepl('FID|IID|^SCORE', names(scores_pcs_resid)), with=F]
  
  # Save mean and SD of PGS residuals in 'trans' population
  # This should be approximately mean = 0 and SD = 1, but save as a sanity check
  scores_pcs_resid_scale<-score_mean_sd(scores=scores_pcs_resid)
  fwrite(scores_pcs_resid_scale, paste0(output,'-TRANS.scale'), sep=' ', col.names=T, quote=F)
  
  # Save PGS ~ PC models
  saveRDS(mod_list, file = paste0(output,'-TRANS.model.rds'))
  
  # Save TRANS PGS in reference
  fwrite(scores_pcs_resid, paste0(output,'-TRANS.profiles'), sep=' ', na='NA', quote=F)
}

# Remove unused parts of model object for prediction
compact_lm <- function(cm) {
  # just in case we forgot to set
  # y=FALSE and model=FALSE
  cm$y = c()
  cm$model = c()
  
  cm$residuals = c()
  cm$fitted.values = c()
  cm$effects = c()
  cm$qr$qr = c()
  cm$linear.predictors = c()
  cm$weights = c()
  cm$prior.weights = c()
  cm$data = c()
  cm
}

# Adjust PGS for ancestry using reference PC models with parallel processing
score_adjust <- function(score, pcs, ref_model, chunk_size = 10) {
  original_order <- names(score)
  
  # Ensure 'pcs' keyed for fast lookup
  setkey(pcs, FID, IID)
  
  # List of score columns
  score_cols <- setdiff(names(score), c("FID", "IID"))
  
  # Split into chunks for memory efficiency
  score_chunks <- split(score_cols, ceiling(seq_along(score_cols) / chunk_size))
  
  # Match PC rows by FID/IID directly (minimal memory usage)
  matched_idx <- pcs[score[, .(FID, IID)], which = TRUE, nomatch = 0]
  
  # Process each chunk sequentially to manage RAM
  for (chunk in score_chunks) {
    
    # Run in parallel across columns within chunk
    chunk_results <- mclapply(chunk, function(col_name) {
      cat("Processing:", col_name, "\n")
      
      # Fetch models for current score column
      mean_model <- ref_model[[col_name]]$mean_model
      var_model  <- ref_model[[col_name]]$var_model
      
      # Pre-allocate adjusted score vector
      adjusted_score <- rep(NA_real_, nrow(score))
      
      # Predict mean and variance using matched PCs
      if (length(matched_idx) > 0) {
        adjusted_score[matched_idx] <- round(
          (score[[col_name]][matched_idx] - predict(mean_model, newdata = pcs[matched_idx])) / 
            sqrt(exp(predict(var_model, newdata = pcs[matched_idx]))),
          3
        )
      }
      
      adjusted_score
    }, mc.cores = min(getDoParWorkers(), 10))
    
    # Write results directly back to 'score' object by reference
    for (idx in seq_along(chunk)) {
      set(score, j = chunk[idx], value = chunk_results[[idx]])
    }
    
    # Explicitly clean memory after each chunk
    rm(chunk_results)
    gc()
    
  }
  
  setcolorder(score, original_order)
  
  return(score)
}

# Helper function to calculate relative weights for a single file from LEOPARD
cal_avg_rel_weights <- function(path){
  weights_file <- fread(path)
  weights_non_0 <-  ifelse(weights_file$Weights < 0, 0, weights_file$Weights)
  rel_weights <- weights_non_0/(sum(weights_non_0))
  return(rel_weights)
}

# Function to calculate average weights across replications from LEOPARD
calculate_avg_weights <- function(populations, leopard_dir, log_file = NULL) {
  avg_weights <- list()
  
  # Iterate over populations
  for (targ_pop in populations) {
    # Define the file prefix for the population
    weights_prefix <- paste0(leopard_dir, '/weights_', targ_pop, '/output_LEOPARD_weights_rep')
    
    # Calculate relative weights for each replication
    rel_weights_list <- lapply(1:4, function(i) {
      cal_avg_rel_weights(paste0(weights_prefix, i, ".txt"))
    })
    
    # Calculate the average weights across replications
    avg_weights[[targ_pop]] <- as.numeric(colMeans(do.call(rbind, rel_weights_list), na.rm = TRUE))
  }
  
  log_add(log_file = log_file, message = '------------------------')
  for(i in names(avg_weights)){
    log_add(log_file = log_file, message = paste0("LEOPARD weights - ", i, " target: "))
    for(j in populations){
      log_add(log_file = log_file, message = paste0(j, ' = ', avg_weights[[i]][which(populations == j)]))
    }
    log_add(log_file = log_file, message = '------------------------')
  }
  
  return(avg_weights)
}

# Centre SNP-weights
centre_weights <- function(score, freq, ref){
  # Sort and flip according to reference data
  score <- map_score(ref = ref, score = score)
  
  # Sort and flip freq according to reference just in case they are different
  freq <- map_score(ref = ref, score = freq)
  
  # Calculate mean genotype dosage and denominator
  freq$MeanGenotype <- 2 * freq$FREQ
  denominator <- sum(freq$MeanGenotype^2)
  
  for(i in names(score)[!(names(score) %in% c('SNP','A1','A2'))]){
    # Calculate mean of PGS
    mean_pgs <- sum(score[[i]] * freq$MeanGenotype)
    
    # Adjust the SNP-weights so PGS is centered
    score[[i]] <- score[[i]] - (mean_pgs / denominator) * freq$MeanGenotype
  }
  return(score)
}

# Linearly combine scores using mixing weights for target population
calculate_weighted_scores <- function(score, targ_pop, mix_weights) {
  if(!all((names(score) %in% c('SNP','A1','A2', paste0('SCORE_targ_', names(mix_weights)))))){
    stop(paste0('score should only contain columns SNP, A1, A2, ', paste(paste0('SCORE_targ_', names(mix_weights)), collapse=', ')))
  }
  score_weighted<-score
  for(disc_pop in names(mix_weights)){
    score_tmp <- score[[paste0('SCORE_targ_', disc_pop)]]
    weight_tmp <- mix_weights[[targ_pop]][which(names(mix_weights) == disc_pop)]
    score_weighted[[paste0('SCORE_targ_', disc_pop)]] <- score_tmp * weight_tmp
  }
  score_combined <- rowSums(score_weighted[, grepl('SCORE_', names(score_weighted)), with = FALSE])
  
  return(score_combined)
}

# Adjust weights to correspond to PGS with SD of 1
adjust_weights <- function(weights, pgs_sd) {
  adjusted_weights <- weights * pgs_sd
  # Normalize weights so they sum to 1
  normalized_weights <- adjusted_weights * (1 / sum(adjusted_weights))
  return(normalized_weights)
}

# Create function to run LRT on allele frequencies
lrt_af_dual <- function(p1, n1, p0, n0) {
  # Convert allele frequencies to counts of alternate alleles
  k1 <- round(2 * n1 * p1)
  k0 <- round(2 * n0 * p0)
  N1 <- 2 * n1
  N0 <- 2 * n0
  
  # Estimate common allele frequency under null
  p_common <- (k1 + k0) / (N1 + N0)
  
  # Log-likelihood under null: same freq
  logL0 <- k1 * log(p_common) + (N1 - k1) * log(1 - p_common) +
    k0 * log(p_common) + (N0 - k0) * log(1 - p_common)
  
  # Log-likelihood under alternative: separate freqs
  logL1 <- k1 * log(p1) + (N1 - k1) * log(1 - p1) +
    k0 * log(p0) + (N0 - k0) * log(1 - p0)
  
  stat <- 2 * (logL1 - logL0)
  pval <- pchisq(stat, df = 1, lower.tail = FALSE)
  
  return(list(stat = stat, p = pval))
}

# New version of lassosum p2cor function that allows for increased precision
z2cor<-function(z, n){
  t <- qt(pnorm(abs(z), lower.tail = FALSE, log.p = TRUE), df = n,
          lower.tail = FALSE, log.p = TRUE) * sign(z)
  return(t/sqrt(n - 2 + t^2))
}

##############
# Quantifying relative PGS R2
##############

readEig <- function(ldDir, block){
  ldfile = file.path(ldDir, paste0("block", block, ".eigen.bin"))
  if(!file.exists(ldfile)){
    stop("can not find LD file", ldfile)
  }
  
  hLD = file(ldfile, "rb")
  m  = readBin(hLD, integer(), n=1, size=4)
  k = readBin(hLD, integer(), n=1, size=4)
  sumLambda = readBin(hLD, numeric(), n=1, size=4)
  thresh = readBin(hLD, numeric(), n=1, size=4)
  lambda = readBin(hLD, numeric(), n=k, size=4)
  mat = readBin(hLD, numeric(), n=m*k, size=4)
  dim(mat) = c(m, k)
  close(hLD)
  
  return(list(m=m, k=k, sumLambda=sumLambda, thresh=thresh, lambda=lambda, U=mat))
}

rel_pgs_r2_missing_eigen <- function(ld_dir,
                                     score_df,   # columns: SNP,A1,A2,<BETA_set1>,<BETA_set2>,
                                     f_miss,     # columns: SNP,F_MISS  (call = 1 - F_MISS),
                                     chr = NULL) {
  
  snp_info <- fread(file.path(ld_dir, "snp.info"))
  if(!is.null(chr)){
    snp_info <- snp_info[snp_info$Chrom == chr,]
  }
  setnames(snp_info, "ID", "SNP")
  snp_info[, S := sqrt(2 * A1Freq * (1 - A1Freq))]
  snp_info[, idx := .I]
  
  # ---- Identify BETA columns, map/flip, reorder to LD order ----
  base_cols <- c("SNP","A1","A2")
  stopifnot(all(base_cols %in% names(score_df)))
  beta_cols <- setdiff(names(score_df), base_cols)
  if (!length(beta_cols)) stop("No BETA columns found after SNP/A1/A2.")
  
  # Remove rows with all-zero BETAs
  score_df <- score_df[, c(base_cols, beta_cols), with = FALSE]
  all_zero <- apply(score_df[, ..beta_cols], 1, function(x) all(x == 0))
  score_df <- score_df[!all_zero]

  # Align score file with snp_info
  score_df <- map_score(ref = snp_info, score = score_df)
  score_df[, idx := .I]
  K <- length(beta_cols)
  
  # Insert missingness information
  score_df <- merge(score_df, f_miss, by = 'SNP', sort = F, all.x =T)
  score_df$F_MISS[is.na(score_df$F_MISS)] <- 1
  score_df$call <- 1 - score_df$F_MISS
  
  # Count non-zero-in-ref per set (for reporting)
  n_in_ref <- vapply(beta_cols, function(x) {
    sum(score_df[[x]] != 0)
  }, integer(1))
  
  # Identify blocks with non-zero BETAs
  nz <- apply(score_df[, beta_cols, with =F], 1, function(x) {
    any(x != 0)
  })
  nz_blocks <- unique(snp_info$Block[nz])
  
  # ---- Parallel loop over blocks ----
  acc <- foreach(b = nz_blocks,
                 .combine = function(a, b) {
                   # elementwise sum combiner for named numeric vectors
                   if (is.null(a)) return(b)
                   a[names(b)] <- a[names(b)] + b[names(b)]
                   a
                 },
                 .inorder = FALSE,
                 .export = c("readEig"),
                 .packages = "data.table") %dopar% {
                   
   idx_b <- which(snp_info$Block == b)
   if (!length(idx_b)) return(setNames(numeric(4*K), NULL))
   
   # Skip reading eigen if all betas zero in this block
   any_nonzero <- FALSE
   for (k in seq_len(K)) {
     if (any(score_df[[beta_cols[k]]][idx_b] != 0, na.rm = TRUE)) { any_nonzero <- TRUE; break }
   }
   if (!any_nonzero) {
     out <- numeric(4*K)
     names(out) <- c(paste0("den_", beta_cols),
                     paste0("sig_", beta_cols),
                     paste0("cov_", beta_cols))
     return(out)
   }
   
   eig <- readEig(ld_dir, b)
   
   S_b    <- snp_info$S[idx_b]
   call_b <- score_df$call[idx_b]
   
   # BETA matrix for this block (m_b x K)
   Bmat <- sapply(beta_cols, function(cl) score_df[[cl]][idx_b])
   Bmat[is.na(Bmat)] <- 0
   
   W_b  <- S_b * Bmat
   Wm_b <- (sqrt(call_b) * S_b) * Bmat
   
   Ut_W  <- crossprod(eig$U, W_b)          # (k_eig x K)
   Ut_Wm <- crossprod(eig$U, Wm_b)         # (k_eig x K)
   
   den_b   <- colSums((sqrt(eig$lambda) * Ut_W )^2)
   sig_b   <- colSums((sqrt(eig$lambda) * Ut_Wm)^2)
   cov_b   <- colSums((eig$lambda) * Ut_W * Ut_Wm)

   out <- c( setNames(den_b,   paste0("den_",   beta_cols)),
             setNames(sig_b,   paste0("sig_",   beta_cols)),
             setNames(cov_b,   paste0("cov_",   beta_cols)))
   out
  }
  
  # ---- Reduce totals per set ----
  get_vec <- function(prefix) unname(acc[paste0(prefix, "_", beta_cols)])
  den_sum   <- get_vec("den")
  sig_sum   <- get_vec("sig")
  cov_sum   <- get_vec("cov")

  # ---- Final metric ----
  rel_var <- ifelse(den_sum > 0, sig_sum / den_sum, NA_real_)
  rel_cor <- ifelse(den_sum > 0 & sig_sum > 0, (cov_sum^2) / (den_sum * sig_sum), NA_real_)
  
  data.table(
    beta_set = beta_cols,
    relative_variance = rel_var,
    relative_R2 = rel_cor,
    den = den_sum, sig = sig_sum, cov = cov_sum,
    n_in_ref = n_in_ref
  )
}

combine_rel_r2 <- function(res_list) {
  stopifnot(length(res_list) > 0)
  
  all_parts <- data.table::rbindlist(res_list, use.names = TRUE, fill = TRUE)
  
  # ensure missing components are zeros before summing
  for (nm in c("den","sig","cov","n_in_ref","n_nz")) {
    if (!nm %in% names(all_parts)) all_parts[, (nm) := 0]
    all_parts[is.na(get(nm)), (nm) := 0]
  }
  
  # Weighted mean missingness across chromosomes (weights = n_nz per chr)
  if (!"mean_nz_miss" %in% names(all_parts)) all_parts[, mean_nz_miss := NA_real_]
  
  agg <- all_parts[, .(
    den   = sum(den,   na.rm = TRUE),
    sig   = sum(sig,   na.rm = TRUE),
    cov   = sum(cov,   na.rm = TRUE),
    n_in_ref = sum(n_in_ref, na.rm = TRUE),
    n_nz     = sum(n_nz,     na.rm = TRUE),
    mean_nz_miss = {
      w <- n_nz; x <- mean_nz_miss
      if (sum(w, na.rm = TRUE) > 0) stats::weighted.mean(x, w, na.rm = TRUE) else NA_real_
    }
  ), by = beta_set]
  
  agg[, relative_variance := fifelse(den > 0, sig / den, NA_real_)]
  agg[, relative_R2 := fifelse(den > 0 & sig > 0, (cov^2) / (den * sig), NA_real_)]
  
  data.table::setcolorder(agg, c("beta_set","relative_variance","relative_R2","den","sig","cov","n_in_ref","n_nz","mean_nz_miss"))
  agg[]
}

# LD-naive method
# LD-naive relative R^2 using only allele frequencies and call rate
# f_miss must have: SNP, F_MISS  (q = 1 - F_MISS)
rel_pgs_r2_missing_af <- function(ld_dir,
                                  score_df,   # cols: SNP, A1, A2, <BETA_set1>, <BETA_set2>, ...
                                  f_miss,     # cols: SNP, F_MISS
                                  chr = NULL) {
  
  # ---- Load AFs ----
  snp_info <- fread(file.path(ld_dir, "snp.info"))
  if (!is.null(chr)) snp_info <- snp_info[Chrom == chr]
  setnames(snp_info, "ID", "SNP")
  snp_info[, S := sqrt(2 * A1Freq * (1 - A1Freq))]
  
  # ---- Identify beta columns; drop all-zero rows ----
  base_cols <- c("SNP","A1","A2")
  stopifnot(all(base_cols %in% names(score_df)))
  beta_cols <- setdiff(names(score_df), base_cols)
  if (!length(beta_cols)) stop("No BETA columns found after SNP/A1/A2.")
  score_df <- as.data.table(score_df)[, c(base_cols, beta_cols), with = FALSE]
  score_df <- score_df[rowSums(score_df[, ..beta_cols] != 0, na.rm = TRUE) > 0]
  
  # ---- Align to reference (allele-orientation safe) ----
  score_df <- map_score(ref = snp_info[, .(SNP, A1, A2)], score = score_df)
  
  # ---- Merge in allele SD (S) ----
  score_df <- merge(score_df, snp_info[, .(SNP, S)], by = "SNP", all.x = TRUE, sort = FALSE)
  
  # ---- Add call rate (q = 1 - F_MISS) ----
  fm <- as.data.table(f_miss)[, .(SNP, F_MISS)]
  score_df <- merge(score_df, fm, by = "SNP", all.x = TRUE, sort = FALSE)
  score_df[is.na(F_MISS), F_MISS := 1]
  score_df[, q := pmax(0, pmin(1, 1 - F_MISS))]
  
  # ---- Compute LD-free relative R^2 per beta set ----
  out <- lapply(beta_cols, function(cl) {
    b <- score_df[[cl]]; b[is.na(b)] <- 0
    w2 <- (score_df$S * b)^2
    denom <- sum(w2)
    numer <- sum(w2 * score_df$q)
    data.table(beta_set = cl,
               relative_R2 = if (denom > 0) numer / denom else NA_real_,
               denom = denom, numer = numer,
               n_in_ref = sum(b != 0))
  })
  rbindlist(out)
}

########### Updated function with imputation

## ------------------------------------------------------------------
## Helper: block-wise PGS-impute using eigen-LD
## ------------------------------------------------------------------

pgs_impute_block_eigen <- function(eig,
                                   S_b,
                                   W_b,
                                   Ut_W,
                                   typed_idx,
                                   ridge = 0.1) {

  U      <- eig$U
  lambda <- eig$lambda
  
  m <- length(S_b)
  K <- ncol(W_b)
  
  if (!length(typed_idx) || K == 0L) {
    return(matrix(0, nrow = m, ncol = K))
  }
  
  # --- 1. LHS Construction (Matrix to Invert) ---
  # Construct LD_tt (Target vs Target)
  U_T    <- U[typed_idx, , drop = FALSE]
  UL_T   <- sweep(U_T, 2, sqrt(lambda), "*")
  LD_tt  <- UL_T %*% t(UL_T)
  
  # Add Ridge to LHS (You were already doing this)
  diag(LD_tt) <- diag(LD_tt) + ridge 
  
  chol_LD_tt <- chol(LD_tt)
  
  # --- 2. RHS Construction (Target vs Full Correlation) ---
  # Calculate R_tf * Beta_full
  tmp        <- sweep(Ut_W, 1, lambda, "*")
  LDW_b_mat  <- U %*% tmp                    
  LDW_T_mat  <- LDW_b_mat[typed_idx, , drop = FALSE]
  
  # *** THE FIX: Add Ridge to RHS ***
  # In GCTB, the covariance between a SNP and itself is (1 + ridge).
  # We must add (ridge * W_b) to the RHS for the typed SNPs.
  W_b_typed <- W_b[typed_idx, , drop = FALSE]
  LDW_T_mat <- LDW_T_mat + (ridge * W_b_typed)
  
  # --- 3. Solve System ---
  X <- backsolve(chol_LD_tt,
                 forwardsolve(t(chol_LD_tt), LDW_T_mat))
  
  # --- 4. Back to Beta Scale ---
  beta_imp_T_mat <- sweep(X, 1, S_b[typed_idx], "/")
  
  Bimp_b <- matrix(0, nrow = m, ncol = K)
  Bimp_b[typed_idx, ] <- beta_imp_T_mat
  
  Bimp_b
}

pgs_impute_block_tiled <- function(eig,
                                   S_b,
                                   W_b,
                                   typed_idx,
                                   ridge = 1.5,
                                   window_size = 1000) {
  
  # eig: list(U, lambda)
  # S_b: vector length m (standard deviations)
  # W_b: Matrix m x K (standardized weights)
  # typed_idx: integer vector (indices of SNPs present in target)
  
  m <- nrow(eig$U)
  K <- ncol(W_b)
  
  # Prepare output matrix for imputed weights
  Bimp_b <- matrix(0, nrow = m, ncol = K)
  
  if (!length(typed_idx) || K == 0L) return(Bimp_b)
  
  # 1. Reconstruct Full LD (Approx)
  #    It is more efficient to reconstruct the whole block once 
  #    than to re-compute U * Lam * U' for every small window.
  #    (Assuming m is < ~5000. If m > 10k, this eats RAM).
  
  # U_scaled = U * sqrt(lambda)
  U_scaled <- sweep(eig$U, 2, sqrt(pmax(eig$lambda, 0)), "*")
  
  # R_block = U_scaled %*% t(U_scaled)
  # This matrix is strictly Positive Semi-Definite by construction.
  R_block <- tcrossprod(U_scaled)
  
  # 2. Iterate over Windows (The "Tiling" Strategy)
  #    Matches GCTB: "unsigned numWindow = std::ceil(block->numSnpInBlock/windowSize);"
  
  num_windows <- ceiling(m / window_size)
  
  for (w in 0:(num_windows - 1)) {
    
    # Define window boundaries (1-based index for R)
    start <- (w * window_size) + 1
    end   <- min((w + 1) * window_size, m)
    
    # Indices of ALL SNPs in this window
    win_idxs <- start:end
    
    # Identify which TARGET SNPs fall inside this window
    # intersect() can be slow, vector logic is faster given sorted typed_idx
    typed_in_win <- typed_idx[typed_idx >= start & typed_idx <= end]
    
    # If no target SNPs in this window, we can't impute anything FOR THIS WINDOW.
    # (The weights for missing SNPs in this window remain 0).
    if (length(typed_in_win) == 0) next
    
    # --- Construct Local Matrices ---
    
    # Local indices relative to the full block are 'win_idxs'
    # We need to map 'typed_in_win' to row/col indices of R_block
    
    # R_tt: Correlation among TARGET SNPs in this window
    R_tt <- R_block[typed_in_win, typed_in_win, drop = FALSE]
    
    # Add Ridge to Diagonal
    diag(R_tt) <- diag(R_tt) + ridge
    
    # R_tf: Correlation between TARGET SNPs (rows) and ALL SNPs IN WINDOW (cols)
    # Note: GCTB restricts "Full" SNPs to the window too!
    R_tf <- R_block[typed_in_win, win_idxs, drop = FALSE]
    
    # RHS Construction: R_tf * W_b[window]
    # We only use the original weights from THIS window
    W_win <- W_b[win_idxs, , drop = FALSE]
    RHS   <- R_tf %*% W_win
    
    # Add Ridge for the Typed SNPs (GCTB Logic)
    # We need to match the rows of RHS (which correspond to typed_in_win)
    W_typed <- W_b[typed_in_win, , drop = FALSE]
    RHS <- RHS + (ridge * W_typed)
    
    # --- Solve Local System ---
    
    # Since R_tt is a principal submatrix of a PSD matrix + Ridge, 
    # it is strictly Positive Definite. Cholesky will succeed.
    chol_R_tt <- chol(R_tt)
    
    X <- backsolve(chol_R_tt, 
                   forwardsolve(t(chol_R_tt), RHS))
    
    # --- Save Results ---
    
    # Convert back to Beta scale: X / S[typed]
    beta_res <- sweep(X, 1, S_b[typed_in_win], "/")
    
    # Store in the output matrix
    Bimp_b[typed_in_win, ] <- beta_res
  }
  
  return(Bimp_b)
}

####
# PGS-imputation with continuous weight based on quality
####

pgs_impute_block_continuous_eigen <- function(eig,
                                              S_b,       # length m_b: sqrt(2 p (1-p))
                                              Bmat,      # m_b x K: baseline betas
                                              q_b,       # length m_b: quality in [0,1]
                                              q_min = 0, # min quality to be a recipient
                                              ridge_base = 1e-4,
                                              ridge_q = 0) {
  # eig: list(U, lambda) from readEig()
  # S_b: numeric, length m_b
  # Bmat: numeric matrix (m_b x K), baseline betas per block
  # q_b: numeric, length m_b, quality measure in [0,1]
  # q_min: threshold for being allowed as a *recipient* of redistributed effect
  # ridge_base: base ridge term on LD_tt diag
  # ridge_q: additional ridge proportional to (1 - q_T) to discourage low-q recipients
  
  U      <- eig$U
  lambda <- eig$lambda
  
  m <- length(S_b)
  K <- ncol(Bmat)
  if (m == 0L || K == 0L) {
    return(list(
      B_adj       = matrix(0, nrow = m, ncol = K),
      B_keep      = matrix(0, nrow = m, ncol = K),
      B_donor     = matrix(0, nrow = m, ncol = K),
      B_donor_imp = matrix(0, nrow = m, ncol = K)
    ))
  }
  
  # 1) Split into "keep" and "donor" components
  #    beta_keep_i = q_i * beta_i
  #    beta_donor_i = (1-q_i) * beta_i
  B_keep  <- sweep(Bmat, 1L, q_b,     "*")
  B_donor <- sweep(Bmat, 1L, (1 - q_b), "*")
  
  # If no donor signal at all, return baseline (all kept)
  if (all(B_donor == 0)) {
    return(list(
      B_adj       = B_keep,
      B_keep      = B_keep,
      B_donor     = B_donor,
      B_donor_imp = matrix(0, nrow = m, ncol = K)
    ))
  }
  
  # 2) W_donor = S * beta_donor
  W_donor <- S_b * B_donor                         # (m x K)
  
  # Transform donor into eigen space
  Ut_W_donor <- crossprod(U, W_donor)              # (k x K)
  
  # 3) Define "recipient" SNPs: high enough quality to receive redistributed effect
  typed_idx <- which(q_b >= q_min)
  if (!length(typed_idx)) {
    # No recipients -> can't redistribute; everything stays as keep
    return(list(
      B_adj       = B_keep,
      B_keep      = B_keep,
      B_donor     = B_donor,
      B_donor_imp = matrix(0, nrow = m, ncol = K)
    ))
  }
  
  # 4) Build LD_tt over recipients, with quality-aware ridge if desired
  U_T  <- U[typed_idx, , drop = FALSE]             # (m_T x k)
  UL_T <- sweep(U_T, 2L, sqrt(lambda), "*")        # UL_T = U_T %*% diag(sqrt(lambda))
  LD_tt <- UL_T %*% t(UL_T)                        # (m_T x m_T)
  
  # Ridge on diagonal: base + optional extra for low-q recipients
  q_T <- q_b[typed_idx]
  diag(LD_tt) <- diag(LD_tt) + ridge_base + ridge_q * (1 - q_T)
  
  chol_LD_tt <- chol(LD_tt)
  
  # 5) Compute LD * W_donor (on full block), then restrict to recipients
  tmp          <- sweep(Ut_W_donor, 1L, lambda, "*") # (k x K)
  LDW_donor    <- U %*% tmp                          # (m x K)
  LDW_T_donor  <- LDW_donor[typed_idx, , drop = FALSE]  # (m_T x K)
  
  # 6) Solve LD_tt %*% X = LDW_T_donor  for all K simultaneously
  X <- backsolve(chol_LD_tt,
                 forwardsolve(t(chol_LD_tt), LDW_T_donor))
  
  # Convert back to per-allele scale: beta_imp_T = X / S_T
  beta_imp_T <- sweep(X, 1L, S_b[typed_idx], "/")  # (m_T x K)
  
  # Put donor-imputed betas into full-length matrix
  B_donor_imp <- matrix(0, nrow = m, ncol = K)
  B_donor_imp[typed_idx, ] <- beta_imp_T
  
  # 7) Final adjusted betas: keep local part + redistributed donor part
  B_adj <- B_keep + B_donor_imp
  
  list(
    B_adj       = B_adj,
    B_keep      = B_keep,
    B_donor     = B_donor,
    B_donor_imp = B_donor_imp
  )
}

# ------------------------------------------------------------------
# Main: relative variance & R2 (masked + optional imputed) + imputed betas
# ------------------------------------------------------------------

eval_pgs_recovery <- function(ld_dir,
                                     score_df,   # cols: SNP, A1, A2, <BETA_set1>, <BETA_set2>, ...
                                     f_miss,     # cols: SNP, F_MISS  (call = 1 - F_MISS)
                                     chr = NULL,
                                     impute = FALSE) {
  # ld_dir: directory with snp.info and eigen LD blocks (readEig)
  # score_df: score file(s) aligned roughly to LD reference (will be map_score()'d)
  # f_miss: missingness info per SNP in target (F_MISS); if NA -> treated as missing
  # chr: optional chromosome filter for snp.info
  # impute: if TRUE, perform PGS-impute and estimate its performance; also return imputed weights
  
  require(data.table)
  
  ## ---- Read and prepare SNP info ----
  snp_info <- data.table::fread(file.path(ld_dir, "snp.info"))
  if (!is.null(chr)) {
    snp_info <- snp_info[Chrom == chr]
  }
  data.table::setnames(snp_info, "ID", "SNP")
  snp_info[, S := sqrt(2 * A1Freq * (1 - A1Freq))]
  snp_info[, idx := .I]
  
  ## ---- Identify BETA columns, drop all-zero rows, align with LD order ----
  base_cols <- c("SNP", "A1", "A2")
  stopifnot(all(base_cols %in% names(score_df)))
  beta_cols <- setdiff(names(score_df), base_cols)
  if (!length(beta_cols)) stop("No BETA columns found after SNP/A1/A2.")
  
  score_df <- as.data.table(score_df)[, c(base_cols, beta_cols), with = FALSE]
  
  # Drop rows where all betas are zero
  all_zero <- apply(score_df[, ..beta_cols], 1L, function(x) all(x == 0))
  score_df <- score_df[!all_zero]
  
  # Align score_df to snp_info (alleles + order)
  score_df <- map_score(ref = snp_info, score = score_df)
  score_df[, idx := .I]
  
  K <- length(beta_cols)
  
  ## ---- Insert missingness information ----
  f_miss <- as.data.table(f_miss)
  score_df <- merge(score_df, f_miss, by = "SNP", sort = FALSE, all.x = TRUE)
  score_df$F_MISS[is.na(score_df$F_MISS)] <- 1
  score_df[, call := 1 - F_MISS]
  
  ## ---- Count non-zero-in-ref per set (for reporting) ----
  n_in_ref <- vapply(beta_cols, function(x) {
    sum(score_df[[x]] != 0)
  }, integer(1))
  
  ## ---- Identify blocks with any non-zero BETA ----
  nz <- apply(score_df[, beta_cols, with = FALSE], 1L, function(x) any(x != 0))
  nz_blocks <- unique(snp_info$Block[nz])
  
  ## ---- Parallel loop over blocks ----
  acc <- foreach::foreach(
    b        = nz_blocks,
    .packages = "data.table",
    .export   = c("readEig", "pgs_impute_block_eigen")
  ) %dopar% {
    
    print(b)
    
    idx_b <- which(snp_info$Block == b)
    if (!length(idx_b)) {
      return(list(metrics = numeric(0), imp = data.table()))
    }
    
    # Quick check: any non-zero betas at all in this block?
    any_nonzero <- FALSE
    for (k in seq_len(K)) {
      if (any(score_df[[beta_cols[k]]][idx_b] != 0, na.rm = TRUE)) {
        any_nonzero <- TRUE
        break
      }
    }
    if (!any_nonzero) {
      metrics <- c(
        setNames(rep(0, K), paste0("den_",      beta_cols)),
        setNames(rep(0, K), paste0("sig_",      beta_cols)),
        setNames(rep(0, K), paste0("cov_",      beta_cols)),
        setNames(rep(0, K), paste0("sig_imp_",  beta_cols)),
        setNames(rep(0, K), paste0("cov_imp_",  beta_cols))
      )
      return(list(metrics = metrics, imp = data.table()))
    }
    
    eig <- readEig(ld_dir, b)
    
    S_b    <- snp_info$S[idx_b]
    call_b <- score_df$call[idx_b]
    
    # BETA matrix (m_b x K)
    Bmat <- sapply(beta_cols, function(cl) score_df[[cl]][idx_b])
    Bmat[is.na(Bmat)] <- 0
    
    W_b  <- S_b * Bmat
    Wm_b <- (sqrt(call_b) * S_b) * Bmat
    
    Ut_W  <- crossprod(eig$U, W_b)   # (k x K)
    Ut_Wm <- crossprod(eig$U, Wm_b)  # (k x K)
    
    den_b   <- colSums((sqrt(eig$lambda) * Ut_W )^2)
    sig_b   <- colSums((sqrt(eig$lambda) * Ut_Wm)^2)
    cov_b   <- colSums((eig$lambda) * Ut_W * Ut_Wm)

    # ---- Optional: PGS-impute within block ----
    if (impute) {
      typed_idx <- which(call_b > 0)
      
      if (length(typed_idx)) {
        Bimp_b <- pgs_impute_block_tiled(
          eig         = eig,
          S_b         = S_b,
          W_b         = W_b,
          typed_idx   = typed_idx,
          ridge       = 1.5, # Selected based on evaluation in UKB to give well calibrated weights
          window_size = 1000  # GCTB Default
        )

        pgs_impute_block_eigen
        
        Wimp_b  <- S_b * Bimp_b
        Ut_Wimp <- crossprod(eig$U, Wimp_b)
        
        sig_imp_b <- colSums((sqrt(eig$lambda) * Ut_Wimp)^2)
        cov_imp_b <- colSums((eig$lambda) * Ut_W * Ut_Wimp)
        
        imp <- as.data.table(Bimp_b)
        data.table::setnames(imp, paste0("BETA_IMP_", beta_cols))
        imp[, idx := idx_b]
      } else {
        sig_imp_b <- rep(0, K)
        cov_imp_b <- rep(0, K)
        imp       <- data.table()
      }
      
    } else {
      sig_imp_b <- rep(0, K)
      cov_imp_b <- rep(0, K)
      imp       <- data.table()
    }
    
    metrics <- c(
      setNames(den_b,      paste0("den_",      beta_cols)),
      setNames(sig_b,      paste0("sig_",      beta_cols)),
      setNames(cov_b,      paste0("cov_",      beta_cols)),
      setNames(sig_imp_b,  paste0("sig_imp_",  beta_cols)),
      setNames(cov_imp_b,  paste0("cov_imp_",  beta_cols))
    )
    
    list(metrics = metrics, imp = imp)
  }
  
  ## ---- Reduce metrics across blocks ----
  metrics_list <- lapply(acc, `[[`, "metrics")
  imp_list     <- lapply(acc, `[[`, "imp")
  
  metrics_sum <- Reduce(function(a, b) {
    if (length(a) == 0) return(b)
    a[names(b)] <- a[names(b)] + b
    a
  }, metrics_list, init = numeric(0))
  
  get_vec <- function(prefix) {
    nm <- paste0(prefix, "_", beta_cols)
    out <- metrics_sum[nm]
    # metrics_sum could be named numeric; unname it
    unname(out)
  }
  
  den_sum     <- get_vec("den")
  sig_sum     <- get_vec("sig")
  cov_sum     <- get_vec("cov")
  sig_imp_sum <- get_vec("sig_imp")
  cov_imp_sum <- get_vec("cov_imp")
  
  ## ---- Final metrics ----
  # Masked PGS
  rel_var_masked <- ifelse(den_sum > 0, sig_sum / den_sum, NA_real_)
  rel_R2_masked  <- ifelse(den_sum > 0 & sig_sum > 0,
                           (cov_sum^2) / (den_sum * sig_sum),
                           NA_real_)
  
  # Imputed PGS
  if (impute) {
    rel_var_imputed <- ifelse(den_sum > 0, sig_imp_sum / den_sum, NA_real_)
    rel_R2_imputed  <- ifelse(den_sum > 0 & sig_imp_sum > 0,
                              (cov_imp_sum^2) / (den_sum * sig_imp_sum),
                              NA_real_)
  } else {
    rel_var_imputed <- rep(NA_real_, K)
    rel_R2_imputed  <- rep(NA_real_, K)
  }
  
  metrics_dt <- data.table(
    beta_set                 = beta_cols,
    relative_variance_masked = rel_var_masked,
    relative_R2_masked       = rel_R2_masked,
    relative_variance_imputed = rel_var_imputed,
    relative_R2_imputed       = rel_R2_imputed,
    den         = den_sum,
    sig_masked  = sig_sum,
    cov_masked  = cov_sum,
    sig_imputed = sig_imp_sum,
    cov_imputed = cov_imp_sum,
    n_in_ref    = n_in_ref
  )
  
  ## ---- Build imputed weights (if requested) ----
  if (impute) {
    imp_all <- data.table::rbindlist(imp_list, use.names = TRUE, fill = TRUE)
    
    full_imp <- data.table(
      idx = snp_info$idx,
      SNP = snp_info$SNP,
      A1  = snp_info$A1,
      A2  = snp_info$A2
    )
    
    if (nrow(imp_all)) {
      full_imp <- merge(full_imp, imp_all, by = "idx", all.x = TRUE, sort = FALSE)
    }
    
    # Ensure columns exist & fill NA with 0
    for (cl in paste0("BETA_IMP_", beta_cols)) {
      if (!cl %in% names(full_imp)) full_imp[[cl]] <- 0
      full_imp[is.na(get(cl)), (cl) := 0]
    }
    
    return(list(
      metrics    = metrics_dt,
      imp_scores = full_imp
    ))
    
  } else {
    return(list(
      metrics = metrics_dt
    ))
  }
}


## ------------------------------------------------------------------
## Optional: updated combine_rel_r2 for the new metrics format
## ------------------------------------------------------------------

combine_eval_pgs_recovery <- function(res_list) {
  # res_list: list of outputs from eval_pgs_recovery (per chr, etc.)
  # each element: list(metrics = <data.table>, imp_scores = <data.table> or NULL)
  
  require(data.table)
  
  metrics_list <- lapply(res_list, `[[`, "metrics")
  all_parts <- data.table::rbindlist(metrics_list, use.names = TRUE, fill = TRUE)
  
  # Ensure numeric columns exist
  num_cols <- c("den", "sig_masked", "cov_masked",
                "sig_imputed", "cov_imputed",
                "n_in_ref", "n_nz")
  for (nm in num_cols) {
    if (!nm %in% names(all_parts)) all_parts[, (nm) := 0]
    all_parts[is.na(get(nm)), (nm) := 0]
  }
  
  if (!"mean_nz_miss" %in% names(all_parts)) {
    all_parts[, mean_nz_miss := NA_real_]
  }
  
  agg <- all_parts[, .(
    den         = sum(den,         na.rm = TRUE),
    sig_masked  = sum(sig_masked,  na.rm = TRUE),
    cov_masked  = sum(cov_masked,  na.rm = TRUE),
    sig_imputed = sum(sig_imputed, na.rm = TRUE),
    cov_imputed = sum(cov_imputed, na.rm = TRUE),
    n_in_ref    = sum(n_in_ref,    na.rm = TRUE),
    n_nz        = sum(n_nz,        na.rm = TRUE),
    mean_nz_miss = {
      w <- n_nz; x <- mean_nz_miss
      if (sum(w, na.rm = TRUE) > 0) stats::weighted.mean(x, w, na.rm = TRUE) else NA_real_
    }
  ), by = beta_set]
  
  agg[, relative_variance_masked := fifelse(den > 0, sig_masked / den, NA_real_)]
  agg[, relative_R2_masked       := fifelse(den > 0 & sig_masked > 0,
                                            (cov_masked^2) / (den * sig_masked),
                                            NA_real_)]
  agg[, relative_variance_imputed := fifelse(den > 0, sig_imputed / den, NA_real_)]
  agg[, relative_R2_imputed       := fifelse(den > 0 & sig_imputed > 0,
                                             (cov_imputed^2) / (den * sig_imputed),
                                             NA_real_)]
  
  data.table::setcolorder(
    agg,
    c("beta_set",
      "relative_variance_masked", "relative_R2_masked",
      "relative_variance_imputed", "relative_R2_imputed",
      "den", "sig_masked", "cov_masked",
      "sig_imputed", "cov_imputed",
      "n_in_ref", "n_nz", "mean_nz_miss")
  )
  
  agg[]
}

# Create a function for comparing baseline and recovered PGS
rel_pgs_r2_baseline_imputed_eigen <- function(ld_dir,
                                              baseline_score_df,  # SNP,A1,A2,BETA_*
                                              imputed_score_df,   # SNP,A1,A2,BETA_* (same names)
                                              chr = NULL) {
  require(data.table)
  require(foreach)
  
  ## ---- Read LD SNP info ----
  snp_info <- data.table::fread(file.path(ld_dir, "snp.info"))
  if (!is.null(chr)) {
    snp_info <- snp_info[Chrom == chr]
  }
  data.table::setnames(snp_info, "ID", "SNP")
  snp_info[, S := sqrt(2 * A1Freq * (1 - A1Freq))]
  snp_info[, idx := .I]
  
  ## ---- Identify BETA columns and align score files ----
  base_cols <- c("SNP", "A1", "A2")
  stopifnot(all(base_cols %in% names(baseline_score_df)))
  stopifnot(all(base_cols %in% names(imputed_score_df)))
  
  beta_base_cols <- setdiff(names(baseline_score_df), base_cols)
  beta_imp_cols  <- setdiff(names(imputed_score_df),  base_cols)
  
  # Require same BETA columns in both
  if (!setequal(beta_base_cols, beta_imp_cols)) {
    stop("Baseline and imputed score files must have the same BETA columns.")
  }
  beta_cols <- beta_base_cols
  
  if (!length(beta_cols)) stop("No BETA columns found after SNP/A1/A2.")
  
  baseline_score_df <- as.data.table(baseline_score_df)[, c(base_cols, beta_cols), with = FALSE]
  imputed_score_df  <- as.data.table(imputed_score_df)[,  c(base_cols, beta_cols), with = FALSE]
  
  # Align both to LD reference
  baseline_score_df <- map_score(ref = snp_info, score = baseline_score_df)
  imputed_score_df  <- map_score(ref = snp_info, score = imputed_score_df)
  
  # Make sure SNP order is identical and corresponds to snp.info
  # (map_score should guarantee this if implemented consistently)
  stopifnot(identical(baseline_score_df$SNP, snp_info$SNP))
  stopifnot(identical(imputed_score_df$SNP,  snp_info$SNP))
  
  K <- length(beta_cols)
  
  # But for simplicity and consistency with eigen-LD, weâll keep everyone;
  # zeros simply don't contribute to variance/covariance.
  # If you want to trim, you'd also need to trim snp_info and block structure consistently.
  
  ## ---- Identify blocks with any non-zero baseline or imputed betas ----
  nz_baseline <- apply(baseline_score_df[, beta_cols, with = FALSE], 1L,
                       function(x) any(x != 0))
  nz_imputed  <- apply(imputed_score_df[,  beta_cols, with = FALSE], 1L,
                       function(x) any(x != 0))
  nz_any      <- nz_baseline | nz_imputed
  
  nz_blocks <- unique(snp_info$Block[nz_any])
  
  ## ---- Parallel loop over blocks ----
  acc <- foreach::foreach(
    b         = nz_blocks,
    .packages = "data.table",
    .export   = c("readEig")
  ) %dopar% {
    
    print(b)
    
    idx_b <- which(snp_info$Block == b)
    if (!length(idx_b)) {
      return(numeric(0))
    }
    
    # Check if there is any non-zero in this block across baseline or imputed
    any_nonzero <- FALSE
    for (k in seq_len(K)) {
      if (any(baseline_score_df[[beta_cols[k]]][idx_b] != 0, na.rm = TRUE) ||
          any(imputed_score_df[[beta_cols[k]]][idx_b]  != 0, na.rm = TRUE)) {
        any_nonzero <- TRUE
        break
      }
    }
    if (!any_nonzero) {
      # Zero contribution from this block
      out <- c(
        setNames(rep(0, K), paste0("var_base_", beta_cols)),
        setNames(rep(0, K), paste0("var_imp_",  beta_cols)),
        setNames(rep(0, K), paste0("cov_",      beta_cols))
      )
      return(out)
    }
    
    eig <- readEig(ld_dir, b)
    
    # S_b calculated for scaling weights, but NOT for the final variance multiplier
    S_b <- snp_info$S[idx_b] 
    
    # BETA matrices (m_b x K)
    Bbase_b <- sapply(beta_cols, function(cl) baseline_score_df[[cl]][idx_b])
    Bimp_b  <- sapply(beta_cols, function(cl) imputed_score_df[[cl]][idx_b])
    Bbase_b[is.na(Bbase_b)] <- 0
    Bimp_b[is.na(Bimp_b)]   <- 0
    
    # W = S * BETA (Converts to standardized effect sizes)
    Wbase_b <- S_b * Bbase_b
    Wimp_b  <- S_b * Bimp_b
    
    # Transform into eigen space
    Ut_Wbase <- crossprod(eig$U, Wbase_b)  # (k_eig x K)
    Ut_Wimp  <- crossprod(eig$U, Wimp_b)   # (k_eig x K)
    
    var_base_b <- colSums((sqrt(eig$lambda) * Ut_Wbase)^2)
    var_imp_b  <- colSums((sqrt(eig$lambda) * Ut_Wimp )^2)
    cov_b      <- colSums((eig$lambda) * Ut_Wbase * Ut_Wimp)
    
    out <- c(
      setNames(var_base_b, paste0("var_base_", beta_cols)),
      setNames(var_imp_b,  paste0("var_imp_",  beta_cols)),
      setNames(cov_b,      paste0("cov_",      beta_cols))
    )
    out
  }
  
  ## ---- Reduce across blocks ----
  metrics_sum <- Reduce(function(a, b) {
    if (length(a) == 0) return(b)
    a[names(b)] <- a[names(b)] + b
    a
  }, acc, init = numeric(0))
  
  get_vec <- function(prefix) {
    nm <- paste0(prefix, "_", beta_cols)
    unname(metrics_sum[nm])
  }
  
  var_base_sum <- get_vec("var_base")
  var_imp_sum  <- get_vec("var_imp")
  cov_sum      <- get_vec("cov")
  
  ## ---- Final metrics ----
  rel_var <- ifelse(var_base_sum > 0, var_imp_sum / var_base_sum, NA_real_)
  rel_R2  <- ifelse(var_base_sum > 0 & var_imp_sum > 0,
                    (cov_sum^2) / (var_base_sum * var_imp_sum),
                    NA_real_)
  
  data.table(
    beta_set     = beta_cols,
    var_baseline = var_base_sum,
    var_imputed  = var_imp_sum,
    relative_variance = rel_var,
    cor2_baseline_imputed = rel_R2
  )
}

